{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2f9d158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from math import floor, ceil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import hamming_loss, make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, KFold, RandomizedSearchCV\n",
    "\n",
    "from ddi.utils import df_optimized, get_data_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e3ac6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    '''retrieve and clean the final_dataset'''\n",
    "    df = pd.read_csv(get_data_filepath('final_dataset.csv'))\n",
    "    df.drop(columns =[col for col in df.columns if 'Unnamed' in col], inplace = True)\n",
    "    df.drop(columns = ['86'], inplace = True)\n",
    "    df = df_optimized(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0b7a0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b451e8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    X = abs(df[df.columns[89:]])  \n",
    "    y = df[df.columns[3:89]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "    \n",
    "    '''scaling X_train'''\n",
    "    std_scaler = StandardScaler()\n",
    "    std_scaler.fit(X_train)\n",
    "    X_train = pd.DataFrame(std_scaler.transform(X_train), columns = X.columns)\n",
    "    \n",
    "    '''Find the number of principal components that explains 80% of the variation in the dataset'''\n",
    "    pca = PCA()\n",
    "    pca.fit(X_train)\n",
    "\n",
    "    pc = 0  \n",
    "    for index, var in enumerate(np.cumsum(pca.explained_variance_ratio_)):\n",
    "        if var > 0.80:\n",
    "            pc = index\n",
    "            print(f\" 80% of the variation can be explained by {index} principal components\")\n",
    "            break\n",
    "            \n",
    "    '''transform both X_train and Y_train'''\n",
    "    pca = PCA(n_components = pc)\n",
    "    pca.fit(X_train)\n",
    "    x_train = pca.transform(X_train)\n",
    "    \n",
    "    X_test = std_scaler.transform(X_test)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "27c05990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80% of the variation can be explained by 46 principal components\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = pre_process(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "901ec0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Modified Hamming_loss\"\"\"\n",
    "hamming_loss_neg = make_scorer(lambda y_true, y_pred: 1-hamming_loss(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5df196ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid_search(X_train,y_train):\n",
    "    \"\"\"input X and Y train to return best parameters through a random grid search\"\"\"\n",
    "    rfc = RandomForestClassifier()\n",
    "    clf = MultiOutputClassifier(rfc)\n",
    "    \n",
    "    n_estimators = [int(x) for x in np.linspace(start = 10, stop = 20, num = 10)]\n",
    "    max_depth = [int(x) for x in np.linspace(10, 20, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    \n",
    "    random_grid = {'estimator__n_estimators': n_estimators,\n",
    "                   'estimator__max_depth': max_depth,\n",
    "                   'estimator__min_samples_split': min_samples_split,\n",
    "                   'estimator__min_samples_leaf': min_samples_leaf\n",
    "                  }\n",
    "    \n",
    "    search = RandomizedSearchCV(\n",
    "        estimator = clf, \n",
    "        n_iter = 2, # change to 50\n",
    "        param_distributions = random_grid, \n",
    "        cv = 2, \n",
    "        n_jobs = -3, \n",
    "        verbose = 1, \n",
    "        scoring = hamming_loss_neg)\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a78bd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train,y_train):\n",
    "    \n",
    "    \n",
    "    rfc = RandomForestClassifier()\n",
    "    clf = MultiOutputClassifier(rfc)\n",
    "    \n",
    "    \"\"\"return the random search best parameters \"\"\"\n",
    "    random_search_params = random_grid_search(X_train,y_train)\n",
    "\n",
    "    ''' create a range of values for each hyperparameter based on random search best parameters'''\n",
    "    param_grid = {}\n",
    "    for k,v in random_search_params.items():\n",
    "        param_grid[k] = [floor(v*0.9), v, ceil(v*1.1)]\n",
    "\n",
    "\n",
    "    search = GridSearchCV(\n",
    "        estimator = clf, \n",
    "        param_grid = param_grid, \n",
    "        cv = 2, \n",
    "        n_jobs = -2, \n",
    "        verbose = 1, \n",
    "        scoring = hamming_loss_neg)\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7067f72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Fitting 2 folds for each of 81 candidates, totalling 162 fits\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "23eaeb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator__max_depth': 15,\n",
       " 'estimator__min_samples_leaf': 5,\n",
       " 'estimator__min_samples_split': 9,\n",
       " 'estimator__n_estimators': 20}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d889f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
